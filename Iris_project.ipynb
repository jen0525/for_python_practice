#!/usr/bin/env python
# coding: utf-8

# # 6-1. 프로젝트 (1) load_digits : 손글씨를 분류해 봅시다

# (1) 필요한 모듈 import하기

# In[1]:


from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report


# (2) 데이터 준비

# In[2]:


digits = load_digits()


# (3) 데이터 이해하기

# In[3]:


digits.keys() #digits 데이터셋에 담긴 정보 종류 확인


# ◎ Feature Data 지정하기

# In[4]:


# keys에서 확인한 정보 중 data를 따로 digits_data 변수에 저장
digits_data = digits.data

print(digits_data.shape) 
# shape는 배열의 형상정보를 출력
# 1797개의 데이터가 각각 64개의 정보를 지니고 있음


# In[5]:


digits_data[0]
# 1793개의 데이터 중 첫 번째 데이터 출력
# 총 64개의 숫자 


# ◎ Label Data 지정하기

# In[6]:


digits_label = digits.target
# keys에서 확인한 정보 중 target을 따로 digits_label 변수에 저장

print(digits_label.shape)
digits_label[:20]
# 1797개의 숫자만 가지고 있음


# ◎ Target Names 출력해 보기

# In[7]:


digits.target_names
#0에서 9까지의 숫자


# ◎ 데이터 Describe 해 보기

# In[8]:


print(digits.DESCR)


# (4) train, test 데이터 분리

# In[10]:


import pandas as pd


# In[11]:


type(digits_data)


# In[12]:


digits_df = pd.DataFrame(data= digits_data, columns =digits.feature_names)
# 1797개 데이터가 각각 64개의 정보를 가지고 있던 digits_data를 
# digits.feature_names을 컬럼명으로 하는 DataFrame 자료형으로 변환해서 digits_df 변수에 저장
# (원래 digits_data는 배열(np.array) 자료형이었음!)
digits_df


# In[13]:


digits_df["label"] = digits.target
# digits_df에 label이라는 컬럼을 새로 추가
# 1797개의 숫자로 이루어져 있었던 digits.target를 label 컬럼에 채워넣기
#이제 각 숫자 이미지(픽셀 데이터)와 정답(레이블)이 함께 저장된 구조
#즉, "label" 컬럼이 추가됨으로써 각 숫자 이미지가 실제 어떤 숫자인지 정답을 알 수 있게 됨
digits_df


# In[21]:


X_train, X_test, y_train, y_test = train_test_split(digits_data, 
                                                    digits_label, 
                                                    test_size=0.2, 
                                                    random_state=7)
# 나눠야 할 데이터(문제지, X): digits_data
# 데이터의 라벨(정답, y): digits_label
# digits_data와 digits_label를 각각 train:test = 8:2의 비율로 잘라서 
# X_train, X_test, y_train, y_test에 저장


print('X_train 개수: ', len(X_train),', X_test 개수: ', len(X_test))
# len은 배열의 길이를 출력


# In[15]:


X_train.shape, y_train.shape
# train의 형상정보 확인


# In[16]:


X_test.shape, y_test.shape
# test의 형상정보 확인


# In[17]:


y_train, y_test
# label이 잘 분리되었는지 확인


# (5) 다양한 모델로 학습시켜보기

# ◎ Decision Tree 사용해 보기

# In[18]:


from sklearn.tree import DecisionTreeClassifier 
# sklearn.tree 패키지에서 의사결정트리 모델 import

decision_tree = DecisionTreeClassifier(random_state=32) 
# random_state : 재현가능하도록 난수의 초기값 32로 설정
print(decision_tree._estimator_type)


# In[19]:


# 학습데이터 X_train, y_train로 의사결정나무 모델로 학습하기
decision_tree.fit(X_train, y_train)


# In[22]:


y_pred = decision_tree.predict(X_test)
y_pred


# In[23]:


# 진짜 정답
y_test


# In[24]:


# 실제 정답 y_test과 예측값 y_pred 비교하여 정확도 측정하기
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
accuracy


# In[25]:


print(classification_report(y_test, y_pred))


# ◎ Random Forest 사용해 보기

# In[26]:


from sklearn.ensemble import RandomForestClassifier #랜덤포레스트라는 분류기를 사용하기 위해 import

X_train, X_test, y_train, y_test = train_test_split(digits_data, # digits 데이터의 data 컬럼
                                                    digits_label, # digits 데이터의 target 컬럼
                                                    test_size=0.2, # test_size : train data와 test data를 몇대몇으로 나눌지 정하는 옵션
                                                    random_state=21) # random_state : 랜덤 패턴의 값을 지정

random_forest = RandomForestClassifier(random_state=32) # RandomForest분류기 객체를 생성
random_forest.fit(X_train, y_train) # 훈련
y_pred = random_forest.predict(X_test) # 예측

print(classification_report(y_test, y_pred)) # 결과 지표를 확인


# ◎ SVM 사용해 보기

# In[27]:


#Support Vector Machine (SVM)
from sklearn import svm #Support Vector Machine을 사용하기 위해 import
svm_model = svm.SVC() # 모델 객체를 만든다.

print(svm_model._estimator_type) # 이 모델이 어떤 타입인지 확인한다.


# In[28]:


svm_model.fit(X_train,y_train)
y_pred = svm_model.predict(X_test)

print(classification_report(y_test, y_pred))


# ◎ SGD Classifier 사용해 보기

# In[30]:


#Stochastic Gradient Descent Classifier (SGDClassifier)
from sklearn.linear_model import SGDClassifier #선형분류기인 SGDClassifier를 사용하기 위한 import
sgd_model = SGDClassifier() # 모델 객체 생성

print(sgd_model._estimator_type) # 이 모델의 타입을 확인


# In[31]:


sgd_model.fit(X_train,y_train)
y_pred = sgd_model.predict(X_test)

print(classification_report(y_test, y_pred))


# ◎ Logistic Regression 사용해 보기

# In[32]:


#Logistic Regression
from sklearn.linear_model import LogisticRegression # 선형분류기인 LogisticRegression를 사용하기 위한 import
logistic_model = LogisticRegression() # 모델 객체 생성

print(logistic_model._estimator_type) # 이 모델의 타입을 확인


# In[33]:


logistic_model.fit(X_train,y_train)
y_pred = logistic_model.predict(X_test)

print(classification_report(y_test, y_pred))


# 다중 클래스 분류에서는 특정 클래스의 개별 성능보다는 모델의 전반적인 균형과 성능을 평가하는 것이 중요하다.
# 따라서 Accuracy뿐만 아니라, F1-score 같은 종합적인 지표를 추가적으로 고려하는 것이 바람직하다.


#!/usr/bin/env python
# coding: utf-8

# # 6-2. 프로젝트 (2) load_wine : 와인을 분류해 봅시다

# (1) 필요한 모듈 import하기

# In[1]:


from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report


# (2) 데이터 준비

# In[2]:


from sklearn.datasets import load_wine
wine = load_wine()


# (3) 데이터 이해하기

# In[3]:


wine.keys() #wine 데이터셋에 담긴 정보 종류 확인


# ◎ Feature Data 지정하기

# In[4]:


# keys에서 확인한 정보 중 data를 따로 wine_data 변수에 저장
wine_data = wine.data

print(wine_data.shape) 
# shape는 배열의 형상정보를 출력
# 178개의 데이터가 각각 13개의 정보를 지니고 있음


# In[5]:


wine_data[0]
# 178개의 데이터 중 첫 번째 데이터 출력
# 총  13가지 화학 성분 수치 


# In[6]:


wine.feature_names


# ◎ Label Data 지정하기

# In[7]:


wine_label = wine.target
# keys에서 확인한 정보 중 target을 따로 wine_label 변수에 저장

print(wine_label.shape)
wine_label
# 178개의 숫자만 가지고 있음


# ◎ Target Names 출력해 보기

# In[8]:


wine.target_names


# ◎ 데이터 Describe 해 보기

# In[9]:


print(wine.DESCR)


# (4) train, test 데이터 분리

# In[10]:


import pandas as pd


# In[11]:


type(wine_data)


# In[12]:


wine_df = pd.DataFrame(data= wine_data, columns =wine.feature_names)
# 1797개 데이터가 각각 64개의 정보를 가지고 있던 wine_data를 
# wine.feature_names을 컬럼명으로 하는 DataFrame 자료형으로 변환해서 wine_df 변수에 저장
# (원래 wine_data는 배열(np.array) 자료형이었음!)
wine_df


# In[13]:


wine_df["label"] = wine.target
# wine_df에 label이라는 컬럼을 새로 추가
# 178개의 숫자로 이루어져 있었던 wine.target를 label 컬럼에 채워넣기
wine_df


# In[14]:


X_train, X_test, y_train, y_test = train_test_split(wine_data, 
                                                    wine_label, 
                                                    test_size=0.2, 
                                                    random_state=7,
                                                    stratify=wine_label)
# 나눠야 할 데이터(문제지, X): digits_data
# 데이터의 라벨(정답, y): wine_label
# wine_data와 wine_label를 각각 train:test = 8:2의 비율로 잘라서 
# X_train, X_test, y_train, y_test에 저장


print('X_train 개수: ', len(X_train),', X_test 개수: ', len(X_test))
# len은 배열의 길이를 출력


# In[15]:


X_train.shape, y_train.shape
# train의 형상정보 확인


# In[16]:


X_test.shape, y_test.shape
# test의 형상정보 확인


# In[17]:


y_train, y_test
# label이 잘 분리되었는지 확인


# (5) 다양한 모델로 학습시켜보기

# ◎ Decision Tree 사용해 보기

# In[18]:


from sklearn.tree import DecisionTreeClassifier 
# sklearn.tree 패키지에서 의사결정트리 모델 import

decision_tree = DecisionTreeClassifier(random_state=32) 
# random_state : 재현가능하도록 난수의 초기값 32로 설정
print(decision_tree._estimator_type)


# In[19]:


# 학습데이터 X_train, y_train로 의사결정나무 모델로 학습하기
decision_tree.fit(X_train, y_train)


# In[20]:


y_pred = decision_tree.predict(X_test)
y_pred


# In[21]:


# 진짜 정답
y_test


# In[22]:


# 실제 정답 y_test과 예측값 y_pred 비교하여 정확도 측정하기
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
accuracy


# In[23]:


print(classification_report(y_test, y_pred))


# ◎ Random Forest 사용해 보기

# In[24]:


from sklearn.ensemble import RandomForestClassifier #랜덤포레스트라는 분류기를 사용하기 위해 import

X_train, X_test, y_train, y_test = train_test_split(wine_data, # wine 데이터의 data 컬럼
                                                    wine_label, # wine 데이터의 target 컬럼
                                                    test_size=0.2, # test_size : train data와 test data를 몇대몇으로 나눌지 정하는 옵션
                                                    random_state=7,
                                                    stratify=wine_label) # random_state : 랜덤 패턴의 값을 지정

random_forest = RandomForestClassifier(random_state=32) # RandomForest분류기 객체를 생성
random_forest.fit(X_train, y_train) # 훈련
y_pred = random_forest.predict(X_test) # 예측

print(classification_report(y_test, y_pred)) # 결과 지표를 확인


# ◎ SVM 사용해 보기

# In[25]:


#Support Vector Machine (SVM)
from sklearn import svm #Support Vector Machine을 사용하기 위해 import
svm_model = svm.SVC() # 모델 객체를 만든다.

print(svm_model._estimator_type) # 이 모델이 어떤 타입인지 확인한다.


# In[26]:


svm_model.fit(X_train,y_train)
y_pred = svm_model.predict(X_test)

print(classification_report(y_test, y_pred))


# ◎ SGD Classifier 사용해 보기

# In[27]:


#Stochastic Gradient Descent Classifier (SGDClassifier)
from sklearn.linear_model import SGDClassifier #선형분류기인 SGDClassifier를 사용하기 위한 import
sgd_model = SGDClassifier() # 모델 객체 생성

print(sgd_model._estimator_type) # 이 모델의 타입을 확인


# In[28]:


sgd_model.fit(X_train,y_train)
y_pred = sgd_model.predict(X_test)

print(classification_report(y_test, y_pred))


# ◎ Logistic Regression 사용해 보기

# In[29]:


#Logistic Regression
from sklearn.linear_model import LogisticRegression # 선형분류기인 LogisticRegression를 사용하기 위한 import
logistic_model = LogisticRegression() # 모델 객체 생성

print(logistic_model._estimator_type) # 이 모델의 타입을 확인


# In[30]:


logistic_model.fit(X_train,y_train)
y_pred = logistic_model.predict(X_test)

print(classification_report(y_test, y_pred))


# 다중 클래스 분류에서는 특정 클래스의 개별 성능보다는 모델의 전반적인 균형과 성능을 평가하는 것이 중요하다.
# 따라서 Accuracy뿐만 아니라, F1-score 같은 종합적인 지표를 추가적으로 고려하는 것이 바람직하다.
# Q. 오류의 원인은 label이 불균형하게 분포되어있는 데이터라서 발생하는 것인가?

# In[ ]:



#!/usr/bin/env python
# coding: utf-8

# # 6-3. 프로젝트 (3) load_breast_cancer : 유방암 여부를 진단해 봅시다

# (1) 필요한 모듈 import하기

# In[1]:


from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report


# (2) 데이터 준비

# In[2]:


breast_cancer = load_breast_cancer()


# (3) 데이터 이해하기

# In[3]:


breast_cancer.keys() #breast_cancer 데이터셋에 담긴 정보 종류 확인


# ◎ Feature Data 지정하기

# In[4]:


# keys에서 확인한 정보 중 data를 따로 breast_cancer_data 변수에 저장
breast_cancer_data =breast_cancer.data

print(breast_cancer_data.shape) 
# shape는 배열의 형상정보를 출력
# 569개의 데이터가 각각 30개의 정보를 지니고 있음


# In[5]:


breast_cancer_data[0]
# 569개의 데이터 중 첫 번째 데이터 출력
# 총 30가지 화학 성분 수치 


# In[6]:


breast_cancer.feature_names


# ◎ Label Data 지정하기

# In[7]:


breast_cancer_label = breast_cancer.target
# keys에서 확인한 정보 중 target을 따로 digits_label 변수에 저장

print(breast_cancer_label.shape)
breast_cancer_label
# 569개의 0,1 숫자만 가지고 있음 


# ◎ Target Names 출력해 보기

# In[8]:


breast_cancer.target_names 


# ◎ 데이터 Describe 해 보기

# In[9]:


print(breast_cancer.DESCR)


# (4) train, test 데이터 분리

# In[10]:


import pandas as pd


# In[11]:


type(breast_cancer_data)


# In[12]:


breast_cancer_df = pd.DataFrame(data= breast_cancer_data, columns =breast_cancer.feature_names)
# 569개 데이터가 각각 30개의 정보를 가지고 있던 breast_cancer를 
# breast_cancer.feature_names을 컬럼명으로 하는 DataFrame 자료형으로 변환해서 breast_cancer_df 변수에 저장
# (원래 wine_data는 배열(np.array) 자료형이었음!)
breast_cancer_df


# In[13]:


breast_cancer_df["label"] = breast_cancer.target
# breast_cancer_df에 label이라는 컬럼을 새로 추가
breast_cancer_df


# In[14]:


X_train, X_test, y_train, y_test = train_test_split(breast_cancer_data, 
                                                    breast_cancer_label, 
                                                    test_size=0.2, 
                                                    random_state=7)
# 나눠야 할 데이터(문제지, X): digits_data
# 데이터의 라벨(정답, y): breast_cancer_label
# breast_cancer_data와 breast_cancer_label를 각각 train:test = 8:2의 비율로 잘라서 
# X_train, X_test, y_train, y_test에 저장


print('X_train 개수: ', len(X_train),', X_test 개수: ', len(X_test))
# len은 배열의 길이를 출력


# In[15]:


print(X_train.shape, X_test.shape)  # Train과 Test의 feature 개수가 같은지 확인


# In[16]:


X_train.shape, y_train.shape
# train의 형상정보 확인


# In[17]:


X_test.shape, y_test.shape
# test의 형상정보 확인


# In[18]:


y_train, y_test
# label이 잘 분리되었는지 확인


# (5) 다양한 모델로 학습시켜보기

# ◎ Decision Tree 사용해 보기

# In[19]:


from sklearn.tree import DecisionTreeClassifier 
# sklearn.tree 패키지에서 의사결정트리 모델 import

decision_tree = DecisionTreeClassifier(random_state=32) 
# random_state : 재현가능하도록 난수의 초기값 32로 설정
print(decision_tree._estimator_type)


# In[20]:


# 학습데이터 X_train, y_train로 의사결정나무 모델로 학습하기
decision_tree.fit(X_train, y_train)


# In[21]:


y_pred = decision_tree.predict(X_test)
y_pred


# In[22]:


# 진짜 정답
y_test


# In[23]:


# 실제 정답 y_test과 예측값 y_pred 비교하여 정확도 측정하기
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
accuracy


# In[24]:


print(classification_report(y_test, y_pred))


# ◎ Random Forest 사용해 보기

# In[25]:


from sklearn.ensemble import RandomForestClassifier #랜덤포레스트라는 분류기를 사용하기 위해 import

X_train, X_test, y_train, y_test = train_test_split(breast_cancer_data, # wine 데이터의 data 컬럼
                                                    breast_cancer_label, # wine 데이터의 target 컬럼
                                                    test_size=0.2, # test_size : train data와 test data를 몇대몇으로 나눌지 정하는 옵션
                                                    random_state=21) # random_state : 랜덤 패턴의 값을 지정

random_forest = RandomForestClassifier(random_state=32) # RandomForest분류기 객체를 생성
random_forest.fit(X_train, y_train) # 훈련
y_pred = random_forest.predict(X_test) # 예측

print(classification_report(y_test, y_pred)) # 결과 지표를 확인


# ◎ SVM 사용해 보기

# In[26]:


#Support Vector Machine (SVM)
from sklearn import svm #Support Vector Machine을 사용하기 위해 import
svm_model = svm.SVC() # 모델 객체를 만든다.

print(svm_model._estimator_type) # 이 모델이 어떤 타입인지 확인한다.


# In[27]:


svm_model.fit(X_train,y_train)
y_pred = svm_model.predict(X_test)

print(classification_report(y_test, y_pred))


# ◎ SGD Classifier 사용해 보기

# In[28]:


#Stochastic Gradient Descent Classifier (SGDClassifier)
from sklearn.linear_model import SGDClassifier #선형분류기인 SGDClassifier를 사용하기 위한 import
sgd_model = SGDClassifier() # 모델 객체 생성

print(sgd_model._estimator_type) # 이 모델의 타입을 확인


# In[29]:


sgd_model.fit(X_train,y_train)
y_pred = sgd_model.predict(X_test)

print(classification_report(y_test, y_pred))


# ◎ Logistic Regression 사용해 보기

# In[30]:


#Logistic Regression
from sklearn.linear_model import LogisticRegression # 선형분류기인 LogisticRegression를 사용하기 위한 import
logistic_model = LogisticRegression() # 모델 객체 생성

print(logistic_model._estimator_type) # 이 모델의 타입을 확인


# In[31]:


logistic_model.fit(X_train,y_train)
y_pred = logistic_model.predict(X_test)

print(classification_report(y_test, y_pred))


# 중요도 판단<br>
# 종양이 악성인지 양성인지 즉 암인지 아닌지의 이진 분류에서는 실제 암 환자 중에서 모델이 암으로 정확히 예측한 비율인 recall이 가장 중요하게 평가되는 요소임. 양성을 음성으로 판단하는 False Negative 줄이는 것을 가장 최소화 해야하기 때문

